# Place of exploration

Over the years, I've worked with places where release cycles grow shorter. From integrating all changes into builds a couple of times a week, we've moved over to continuous integration. Each change gets integrated to the latest system and made available for testing as soon as the build automation finishes running. We don't get to test the exactly same thing for a very long time, or if we do, we spend time on something that will not be the final assembly delivered. Similarly, we've moved from giving those assemblies to customers once every six months to continuous delivery and the version in production can change multiple times a day.

In the fast-paced delivery world, we turn to look heavily at automation. As we need to be able to run our tests again and again, and deliver the change as soon as it has been checked in and run through our automated build and test pipeline, surely there is no place for exploratory testing? Or if there is, maybe we just do all of our exploratory testing against a production version? Or maybe on top of all this automation, exploratory testing is a separate activity, happening just before accepting the change into the assembly that gets built and pushed forward? Like a time-box spent on testing whatever risks we saw the change potentially introducing that our automation may not catch?

Think of exploratory testing as a mindset that frames all testing activities - including automation. It's the mindset that suggests that even when we automate, we need to think. That the automation we are creating for continuous testing is a tool, and will be only as good as the thinking that created it. Just like the application it tests.

## An example

We were working in a small team, building the basis for a feature: managing Windows Firewall remotely. There were four of us: Alice and Bob were the programmers assigned at the task. Cecile specialized in end to end test automation. David could read code, but on most days chose not to and through of themselves as the team's exploratory testing specialist.

As the team was taking in the new feature, there was a whole group discussion. The group talked about existing APIs to use for the task at hand, and figured out that the feature had a core. There was the existing Windows Firewall. There was information about rules to add delivered. And those rules needed to be applied, or there would not be the feature. After drawing some boxes on the wall, having discussions about overall and minimal scope, the programmers started their work of writing the application code.

It did not take long until Alice checked in the module frame, and Bob reviewed the pull request accepting the changes making something available that was still just a frame. Alice and Bob paired to build up the basic functionality, leading Cecile and David listening to them bouncing off ideas of what was the right thing to do. As they introduced functionality, they also included unit tests. And as they figured out the next slice of functionality to add, David was noticing how much of exploratory testing the two did between the pair. The unit tests would surprise them on a regular basis, and they took each surprise as an invitation to explore in the context of code. Soon the functionality of adding rules was forming, and the pull requests were accepted within the pair.

Meanwhile, Cecile was setting up possibilities to run the Windows Firewall in a multitude of Windows operating systems. They created a script that introduced five different flavors of Windows that were supposed to be supported for the new functionality to be run as jobs within the continuous integration pipeline. They created libraries that allowed to drive the Windows Firewall in those operating systems, so that one could programmatically see what rules were introduced and shown. Since the team had agreed on the mechanism of how the rules would be delivered from the outside, they also created mechanisms of locally creating rules through the same mechanism.

As soon as the module could be run, Alice and Bob would help out Cecile on getting the test automation scripts running on the module. David also participated as they created the simplest possible thing that should work: adding a rule called "1" that blocked ping and could be verified in system context by running ping before and after. Setting up the scripts on top of Cecile's foundation was straightforward.

Cecile wanted to test their scripts before leaving them running triggered for an hourly repeat for a baseline, and manually started the run on one of their operating systems, visually verifying what was going on. They soon realized that there was a problem they had not anticipated, leaving the list of rules in an unstable state. Visually, things were flickering when the list of rules was looked at through the UI. That was not what happened when rules were added manually. And Cecile had explored enough of the system to know what adding a rule through the existing user interfaces should look like. Something was off.

Cecile, Bob and Alice figured out that the problem was related to naming the rules. If the rule name was less than three characters, there was a problem. So Bob introduced a fix limiting rule's minimum length, Alice approved the change and Cecile changed the rules to have a name longer than three characters. Cecile used Windows Firewall more to figure out different types of rules, and added more cases by exploring what was same and different and made sure they would test things both locally and remotely - end to end.

David had also started exploratory testing the application as soon as there were features available. They had learned that Alice and Bob did not introduce logging right from the start, and as they did, that the log wasn't consistent with other existing logs in how it was named. They were aware of things being built into the automation, and focused their search on things that would expand the knowledge. They identified that there were other ways of introducing rules, or locking the Windows Firewall so that rules could not be introduced through external mechanisms. They would pay attention to rule wizard functionalities in the Windows Firewall, enforcing rules around legal rules, and make notes of those only to realize through testing that Alice and Bob had not considered that all combinations were not legal. Things David would find would not be bugs as the team defined a bug - programming errors - but more of missing functionalities, for lacking information about the execution environment.

David would make lists of tests for Cecile to add to the test automation, and pair with Cecile to add them. As they were pairing, the possibilities of automatically creating a lot of rules triggered their minds and they would try introducing a thousand rules to note performance concerns. And as adding was so much fun, obviously removing some of them would make sense too. They would also add changing rules. And as they were playing with numbers, they realized that they had uncovered a security issue: rules were there to protect, and timings would allow for times of unprotection.

The team built it all to a level they felt was minimal to be delivered outside the organization. Unit tests and test automation allowed for making changes and believing those cases still worked out ok. They could explore around every change, and every idea.

The functionality also included a bit of monitoring, allowing them to see the use of feature in production. After having the feature running in production, monitoring provided extra ideas of what to explore to understand the overall system better.

What this story shows:
  * everyone explores, not everyone calls it exploratory testing even if it is that
  * we explore before be build, during building, while we automate and separately from building and automating, as well as while in production
  * exploration can happen in context of code and in context of the system we're building as well as in context of use in the overall system
  * without exploratory testing, we don't give ourselves the best chances of knowing what we're building

## Separate Tester Role

In software development, we translate ideas to code. When we focus on the idea separately, the translation process separately and the result of the translation process separately. To see things with many dimensions in their full potential, quite much headspace gets occupied. In fact, our ability to focus and think is heavily limited.

It has been a common experience that having people who use their headspace focusing on building the solution separately from people focusing on finding it's weak points and understanding it in context gives us better results in a shorter timeframe. For this to work to its full potential, we need collaboration, as closely knit as possible. Diverse views and focuses bring in the fuller picture when everyone's voices are valued and heard.

With limited headspace for learning, it makes sense we diversify our learning focus. From a personal experience, I can speak on the extensive practice needed to become a tester able to do deep (as opposed to shallow) exploratory testing. Also, I can speak on the experience of needing less of specialized testers in teams in general, as those who do and practice deep exploratory testing can proceed much faster in the modern ways of working where everyone tests and basic quality of the products is not the time consuming obstacle. Where there used to be a need of a tester per programmer, with the new division of labor portions such as 1:10 are common.

## Future is Here, Just Not Evenly Distributed

There are teams where collaboration is taken to a positive extreme and the whole team works together, even through a single computer used to do the work in a group. We call this way of collaboration mob programming. With Mob Programming, the deep exploratory testing as well as a tester perspective's typical insights are contributed as soon as they emerge, while working on the code and solution together.

Just as there are these new closely knit teams, there are teams where people in different roles are highly isolated. There are places in which developers barely test and the organization guides them to leave testing work for a separate role of testers. There are places where developers try to test, but results show that they miss a lot of relevant issues. While the team learns to build software well, the feedback of quality typically provided by testers can be a make-or-break practice for both customer happiness but also team effectiveness.

With places that use long delivery cycles, the reliance of existence of testers is often even higher. With systems having specialized domain knowledge the development teams don't have, it is essential that the domain specialists participate in the testing efforts to make sure the system built is serving the actual needs fulfilling expectations of why it was built in the first place.

No matter which version of software development we live in, exploratory testing plays a key role. It just shows itself in a different packaging in its projectized vs. continuous flow formats. 
